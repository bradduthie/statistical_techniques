# (PART) Randomisation approaches {-}

# Week 11 Overview {-#Week11}

|                 |                                                        |
|-----------------|--------------------------------------------------------|
| **Dates**       | 1 April 2024 - 5 April 2024                            |
| **Reading**     | **Required:** SCIU4T4 Workbook chapter 34              |
|                 | **Recommended:**  None                                 |
|                 | **Suggested:**  None                                    |
|                 | **Advanced:**   @Ernst2004 ([Download](https://projecteuclid.org/journals/statistical-science/volume-19/issue-4/Permutation-Methods-A-Basis-for-Exact-Inference/10.1214/088342304000000396.full))                                    |
| **Lectures**    | 11.1: Introduction to randomisation (18:43 min; [Video](https://stirling.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=543ce3b9-9257-4e66-a52e-b0ed011c7ac2))     |
|                 | 11.2: Assumptions of randomisation (11:03 min; [Video](https://stirling.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=c0a6dac8-cda8-4681-9c22-b0ed011cd941))                      |
|                 | 11.3: Bootstrapping (11:43 min; [Video](https://stirling.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=cfe14de4-1866-4a08-8ce9-b0ed011cffe2))                      |
|                 | 11.4: Monte Carlo (min; [Video](https://stirling.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=266f5350-982a-425d-9319-b0ed011d2de1))                      |
| **Practical**   | Using R ([Chapter 35](#Chapter_35)) |
|                 | Room: Cottrell 2A17                                    |
|                 | Group A: 3 APR 2024 (WED) 09:05-11:55                 |
|                 | Group B: 4 APR 2024 (THU) 12:05-14:55                 |
| **Help hours**  |  Brad Duthie                                           |
|                 | Room: Cottrell 2A21                                    |
|                 | 5 APR 2024 (FRI) 15:05-17:55                          |
| **Assessments** | [Week 11 Practice quiz](https://canvas.stir.ac.uk/courses/14932/quizzes/35440) on Canvas                     |
|                 | [Test 2S](https://canvas.stir.ac.uk/courses/14932/quizzes/35453) on Canvas                     |


# Randomisation {#Chapter_35}

Since introducing statistical hypothesis testing in [Chapter 21](#Chapter_21), this book has steadily introduced statistical tests that can be run for different data types.
In all of these statistical tests, the general idea is the same.
We calculate some test statistic, then compare this test statistic to a predetermined null distribution to find the probability (i.e., p-value) of getting a test statistic as or more extreme than the one we calculated if the null hypothesis were true.
This chapter introduces a different approach.
Instead of using a pre-determined null distribution, we will build the null distribution using our data.
This approach is not one that is often introduced in introductory statistics texts.
It is included here for two reasons.
First, randomisation presents a different way of thinking statistically without introducing an entirely different philosophical or methodological approach such as likelihood [@Edwards1972] or Bayesian statistics [@Lee1997].
Second, it helps reinforce the concept of what null hypothesis testing is and what p-values are.
Before explaining the randomisation approach, it is useful to summarise the parametric hypothesis tests introduced in earlier chapters.


## Summary of parametric hypothesis testing

For the parametric tests introduced in previous chapters, null distributions included the t-distribution, F-distribution, and $\chi^{2}$ distribution.
For the t-tests in [Chapter 22](#Chapter_22), the test statistic was the t-statistic, which we compared to a t-distribution. 
The [one-sample t-test](#one-sample-t-test) compared the mean of some variable ($\bar{x}$) to a specific number ($\mu_{0}$), the [independent samples t-test](#independent-samples-t-test) compared two group means, and the [paired samples t-test](#paired-sample-t-test) compared the mean difference between two paired groups.
All of these tests used the t-distribution and calculated some value of t.
Very low or high values of t at the extreme ends of the t-distribution are unlikely if the null hypothesis is true, so, in a two-tailed test, these are associated with low p-values that lead us to reject the null hypothesis.

For the analysis of variance (ANOVA) tests of [Chapter 24](#Chapter_24), the relevant test statistic was the $F$ statistic, with the F-distribution being the null distribution expected if two variances are equal.
The [one-way ANOVA](#one-way-anova) used the within and among group variances of two or more groups to test the null hypothesis that all group means are equal.
The two-way ANOVA of [Chapter 27](#Chapter_27) extended the framework of the one-way ANOVA, allowing for a second variable of groups.
This made it possible to simultaneously test whether or not the means of two different group types were the same, and whether or not there was an interaction between group types. 
All of these ANOVA tests calculated some value of $F$ and compared it to the F distribution with appropriate degrees of freedom.
Sufficiently high $F$ values were associated with a low p-value and therefore the rejection of the null hypothesis.

The Chi-square tests introduced in [Chapter 29](#Chapter_29) were used to test the frequencies of categorical observations and determine if they matched some expected frequencies ([Chi-square goodness of fit](#chi-squared-goodness-of-fit) test) or were associated in some way with the frequencies of another variable ([Chi-square test of association](#chi-squared-test-of-association)).
In these tests, the $\chi^{2}$ statistic was used and compared to a null $\chi^{2}$ distribution with an appropriate degrees of freedom.
High $\chi^{2}$ values were associated with low p-values and the rejection of the null hypothesis.

For testing the significance of correlation coefficients (see [Chapter 30](#Chapter_30)) and linear regression coefficients (see [Chapter 32](#Chapter_32)), a t-distribution was used.
And an F distribution was used to test for the overall significance of linear regression models.

For these tests, the approach to hypothesis testing was therefore always to use the t-distribution, F distribution, or $\chi^{2}$ distribution in some way.
These distributions are more formally defined in mathematical statistics [@Miller2004], a field of study that uses mathematics to derive the probability distributions that arise from an outcome of random events (e.g., the coin-flipping of [Section 15.1](#an-instructive-example)).
The reason that we use these distributions in statistical hypothesis testing is that they are often quite good at describing the outcomes that we expect when we collect a sample from a population.
But this is not always the case.
Recall that sometimes the assumptions of a particular statistical test were not met.
In this case, a non-parametric alternative was introduced.
The non-parametric test used the ranks of data instead of the actual values (e.g., [Wilcoxon](#wilcoxon-test), [Mann-Whitney U](#mann-whitney-u-test), [Kruskal-Wallis H](#Chapter_26), and [Spearman's rank correlation coefficient](#spearman-rank-correlation-coefficient) tests).
Randomisation uses a different approach.


## Randomisation approach

Randomisation takes a different approach to null hypothesis testing. 
Instead of assuming a theoretical null distribution against which we compare our test statistic, we ask, 'if the ordering of the data we collected was actually random, then what is the probability of getting a test statistic as or more extreme than the one that we actually did'. 
Rather than using a null distribution derived from mathematical statistics, we will build the null distribution by randomising our data in some useful way [@Manly2007]. 
Conceptually, this is often easier to understand because randomisation approaches make it easier to see why the null distribution exists and what it is doing. 
Unfortunately, these methods are more challenging to implement in practice because using them usually requires knowing a bit of coding. 
The best way to get started is with an instructive example.

## Randomisation for hypothesis testing

As in several previous chapters, the dataset used here is inspired by the many species of wasps that lay their eggs in the flowers of the Sonoran Desert rock fig (*Ficus petiolaris*). 
This tree is distributed throughout the Baja peninsula, and in parts of mainland Mexico. 
Fig trees and the wasps that develop inside of them have a fascinating ecology, but for now we will just focus on the morphologies of two closely related species as an example. 
The fig wasps in Figure 35.1 are two unnamed species of the genus *Idarnes*, which we can refer to simply as 'Short-ovipositor 1' (SO1) and 'Short-ovipositor 2' (SO2). 

```{r, echo = FALSE, fig.alt = "A two panel figure is shown, with two black wasps under a microscope slide shown side by side, each of which has an ovipositor about twice its body length.", fig.cap = "Two fig wasp species, roughly 3 mm in length, labelled 'SO1' and 'SO2'. Image modified from Duthie, Abbott, and Nason (2015).", out.width="100%"}
knitr::include_graphics("img/fig_wasps.jpg");
```

The reason that these two species are called 'SO1' and 'SO2' is that there is actually another species that lays its eggs in *F. petiolaris* flowers, one with an ovipositor that is at least twice as long as those in Figure 35.1 [@Duthie2015b]. 

Suppose that we have some data on the lengths of the ovipositors from each species. 
We might want to know whether the mean ovipositor length differs between the two species. Figure 35.2 shows histograms of ovipositor lengths collected from 32 fig wasps, 17 SO1s and 15 SO2s.


```{r, echo = FALSE, fig.alt = "A two panel figure showing a histogram in each panel, the left with a histogram of SO1 ovipositor length and the right with another SO2 ovipositor length. Histograms are roughly normally distributed", fig.cap = "Ovipositor length distributions for two unnamed species of fig wasps, SO1 (A) and SO2 (B), collected from Baja, Mexico."}
sp   <- c(rep("SO1", 17), rep("SO2", 15))
eg   <- c(3.256, 3.133, 3.071, 2.299, 2.995, 2.929, 3.291, 2.658, 3.406, 
          2.976, 2.817, 3.133, 3.000, 3.027, 3.178, 3.133, 3.210, 3.014, 
          2.790, 2.985, 2.911, 2.914, 2.724, 2.967, 2.745, 2.973, 2.560, 
          2.837, 2.883, 2.668, 3.063, 2.639);
wp   <- as.data.frame(cbind(sp, eg));
colnames(wp) <- c("Species", "Ovipositor length (mm)");
sp1 <- as.numeric(as.character(wp[wp[,1]=="SO1",2]));
sp2 <- as.numeric(as.character(wp[wp[,1]=="SO2",2]));
tmo <- t.test(sp1, sp2, var.equal = TRUE);
par(mfrow = c(1,2), mar = c(5, 5, 1, 1));
hist(x = sp1, col = "grey", xlim = c(2, 4), main = "", ylim = c(0, 8),
     breaks = seq(from = 2, to = 4, by = 0.2), xlab = "SO1 ovipositor length");
text(x = 3.7, y = 7.5, labels = "A", cex = 2);
hist(x = sp2, col = "grey", xlim = c(2, 4), main = "", ylim = c(0, 8),
     breaks = seq(from = 2, to = 4, by = 0.2), xlab = "SO2 ovipositor length");
text(x = 3.7, y = 7.5, labels = "B", cex = 2);
N1 <- 17;
N2 <- 15;
tt <- ((((N1 - 1)*var(sp1)) + ((N2 - 1)*var(sp2)))/(N1 + N2 - 2));
bb <- ((N1 + N2)/(N1*N2))
sp <- sqrt(tt * bb);
tv <- (mean(sp1) - mean(sp2))/sp;
```


To test whether or not mean ovipositor length is different between these two fig wasps, our standard approach would be to use an independent samples t-test (see [Section 22.2](#independent-samples-t-test)). 
The null hypothesis would be that the two means are the same, and the alternative (two-sided) hypothesis would be that the two means are not the same. 
We would need to check the assumption that the data are normally distributed, and that both samples have similar variances. 
Assuming that the assumption of normality is not violated (in which case we would need to consider a Mann-Whitney U test), and that both groups had similar variances (if not, we would use Welch's t-test), we could proceed with calculating our t-statistic for pooled sample variance,

$$t_{\bar{y}_{\mathrm{SO1}} - \bar{y}_{\mathrm{SO2}}} = \frac{\bar{y}_{\mathrm{SO1}} - \bar{y}_{\mathrm{SO2}}}{s_{p}}.$$

The $s_{p}$ is just being used as a short-hand to indicate the pooled standard deviation.
For the two species of fig wasps, $\bar{y}_{\mathrm{SO1}} =$ `r mean(sp1)`, $\bar{y}_{\mathrm{SO2}} =$ `r mean(sp2)`, and $s_{p} =$ `r sp`.
We can therefore calculate $t_{\bar{y}_{\mathrm{SO1}} - \bar{y}_{\mathrm{SO2}}}$,

$$t_{\bar{y}_{\mathrm{SO1}} - \bar{y}_{\mathrm{SO2}}} = \frac{`r round(mean(sp1), digits = 3)` - `r round(mean(sp2), digits = 3)`}{`r round(sp, digits = 3)`}.$$

After we calculate our t-statistic as $t_{\bar{y}_{\mathrm{SO1}} - \bar{y}_{\mathrm{SO2}}} =$ `r round(tv, digits = 3)`, we would use the t-distribution to find the p-value (or, rather, get jamovi to do this for us).
Figure 35.3 shows the t-distribution for 30 degrees of freedom with an arrow pointing at the value of $t_{\bar{y}_{\mathrm{SO1}} - \bar{y}_{\mathrm{SO2}}} =$ `r round(tv, digits = 3)`.

```{r, echo = FALSE, fig.alt = "A bell curve of the t-distribution is shown with an arrow pointing downward on the x-axis value around 2.419.", fig.cap = "A t-distribution is shown with a calculated t-statistic of 2.419 indicated with a downward arrow."}
par(mar = c(5, 5, 1.5, 1.5));
xx       <- seq(from = -5, to = 5, by = 0.001);
xx_vals  <- 1:10001;
yy       <- dt(xx, df = 9);
plot(x = xx, y = yy, type = "l", ylim = c(0, 0.48), 
     ylab = "Probability density",  xlab = "t-score", lwd = 2, xlim = c(-3, 3),
     cex.lab = 1.25, cex.axis = 1.25, yaxs = "i");
polygon(c(xx[xx_vals], xx[1], xx[10001]), 
        c(yy[xx_vals], 0, 0), col="grey80");
arrows(x0 = 2.419, x1 = 2.419, y0 = 0.09, y1 = 0.002, lwd = 3);
mbox <- function(x0, x1, y0, y1){
    xx <- seq(from=x0, to=x1, length.out = 100);
    yy <- seq(from=y0, to=y1, length.out = 100);
    xd <- c(rep(x0, 100), xx, rep(x1,100), rev(xx));
    yd <- c(yy, rep(y1,100), rev(yy), rep(y0, 100));
    return(list(x=xd, y=yd));
}
tbox <- mbox(x0 = 2.419 - 0.15, 
             x1 = 2.419 + 0.15, y0 = 5, y1 = 9);
polygon(x=tbox$x, y=tbox$y, lwd=3, border="black", col="white");
text(x = 2.419, y = 0.125, cex = 2, 
     labels = expression(paste(italic(t[bar(y)[SO1] - bar(y)[SO2]]))));
```

If the null hypothesis is true, and $\bar{y}_{\mathrm{SO1}}$ and $\bar{y}_{\mathrm{SO2}}$ are actually sampled from a population with the same mean, then the probability of randomly sampling a value more extreme than `r round(tv, digits = 3)` (i.e., greater than `r round(tv, digits = 3)` or less than `r -1*round(tv, digits = 3)`) is $P =$ `r tmo$p.value`.
We therefore reject the null hypothesis because $P < 0.05$, and we conclude that the mean ovipositor lengths of each species are not the same.

Randomisation takes a different approach to the same problem.
Instead of using the t-distribution in Figure 35.3, we will build our own null distribution using the fig wasp ovipositor length data.
We do it by randomising group identity in the dataset.
The logic is that if there really is no difference between group means, then we should be able to randomly shuffle group identities (species) and get a difference between means that is not far off the one we actually get from the data. 
In other words, what would the difference between group means be if we just mixed up all of the species (so some SO1s become SO2s, some SO2s become SO1s, some stay the same), then calculated the difference between means of the mixed up groups? 
If we just do this once, then we cannot learn much. 
But if we randomly shuffle the groups many, many times (say at least 9999), then we could see what the difference between group means would look like just by chance, i.e., if ovipositor length really was not different between SO1 and SO2. 
We could then compare our actual difference between mean ovipositor lengths to this null distribution in which the difference between groups means really is random (it has to be, we randomised the groups ourselves!).
The idea is easiest to see using an interactive application[^randomisation] that builds a null distribution of differences between the mean of SO1 and SO2 and compares it to the observed difference of $\bar{y}_{\mathrm{SO1}} - \bar{y}_{\mathrm{SO2}} =$ `r round(mean(sp1) - mean(sp2), digits = 3)`.

With modern computing power, we do not need to do this randomisation manually. 
A desktop computer can easily reshuffle the species identities and calculate a difference between means thousands of times in less than a second. 
The histogram below shows the distribution of the difference between species mean ovipositor length if we were to randomly reshuffle groups 99999 times. 

```{r, echo = FALSE, fig.alt = "A histogram with 120 narrow bars that form a bell shaped distribution around a mean of 0. An arrow points out a value of mean SO1 minus mean SO2", fig.cap = "A distribution of the difference between mean species ovipositor lengths in two different species of fig wasps when species identity is randomly shuffled. The true difference between sampled mean SO1 and SO2 ovipositor lengths is pointed out with the black arrow at 0.185. Ovipositor lengths were collected from wasps in Baja, Mexico in 2010."}
iter <- 99999;          
diff <- NULL; 
N    <- dim(wp)[1];
ovi  <- as.numeric(as.character(wp[,2]));
for(i in 1:iter){   
    samp    <- sample(x = wp[,1], size = N, replace = FALSE);
    smpm    <- as.numeric(tapply(X = ovi, INDEX = samp, FUN = mean))
    diff[i] <- smpm[1] - smpm[2];
}
par(mar = c(5, 5, 1, 1));
hist(x = diff, main = "", xlab = "Random difference between means (mm)", 
     cex.axis = 1.25,
     cex.lab = 1.25, col = "grey", breaks = 120);
df1 <- mean(sp1) - mean(sp2);
arrows(x0 = df1, x1 = df1 , y0 = 500, y1 = 10, 
               length = 0.15, lwd = 4, col = "black");
more_extr <- sum(abs(diff) > abs(mean(sp1)-mean(sp2)));
text(x = 0.145, y = 650, cex = 1.5, pos = 4,
     labels = expression(paste(italic(bar(y)[SO1]-bar(y)[SO2]))));
```

Given this distribution of randomised mean differences between species ovipositor lengths, the observed difference of `r round(mean(sp1)-mean(sp2), digits = 3)` appears to be fairly extreme. 
We can quantify how extreme by figuring out the proportion of mean differences in the above histogram that are more extreme than our observed difference (i.e., greater than `r round(mean(sp1)-mean(sp2), digits = 3)` or less than `r -1*round((mean(sp1)-mean(sp2)), digits = 3)`). 
It turns out that only `r more_extr` out of 99999 random mean differences between ovipositor lengths were as or more extreme than our observed difference of `r round(mean(sp1)-mean(sp2), digits = 3)`. 
To express this as a probability, we can simply take the number of differences as or more extreme than our observed difference (including the observed one itself), divided by the total number of differences (again, including the observed one),

$$P = \frac{`r more_extr` + 1}{`r format(iter, scientific = FALSE)` + 1}.$$

When we calculate the above, we get a value of `r (more_extr + 1)/(iter + 1)`. 
Notice that the calculated value is assigned with a '$P$'. 
This is because the value **is a p-value** (technically, an unbiased estimate of a p-value). Consider how close it is to the value of $P =$ `r tmo$p.value` that we got from the traditional t-test. 
Conceptually, we are doing the same thing in both cases; we are comparing a test statistic to a null distribution to see how extreme the differences between groups really are.

## Randomisation assumptions

Recall from [Section 22.4](#assumptions-of-t-tests) the assumptions underlying t-tests, which include (1) data are continuous, (2) sample observations are a random sample from the population, and (3) sample means are normally distributed around the true mean.
With randomisation, we do not need to assume 2 or 3. 
The randomisation approach is still valid even if the data are not normally distributed.
Samples can have different variances.
The observations do not even need to be independent.
The validity of the randomisation approach even when standard assumptions do not apply can be quite useful.

The downside of the randomisation approach is that the statistical inferences that we make are limited to our sample, not the broader population (recall the difference between a sample and population from [Chapter 4](#Chapter_4)). 
Because the randomisation method does not assume that the data are a random sample from the population of interest (as is the case for the traditional t-test), we cannot formally make an inference about the difference between populations from which the sample was made. 
This is not necessarily a problem in practice.
It is only relevant in terms of the formal assumptions of the model. 
Once we run our randomisation test, it might be entirely reasonable to argue verbally that the results of our randomisation test can generalise to our population of interest.
In other words, we can argue that the difference between groups in the sample reflects a difference in the populations from which the sample came [@Ludbrook1998; @Ernst2004].

[^randomisation]: [https://bradduthie.github.io/stats/app/randomisation/](https://bradduthie.github.io/stats/app/randomisation/)

## Bootstrapping

We can also use randomisation to calculate confidence intervals for a variable of interest. Remember from [Chapter 18](#Chapter_18) the traditional way to calculate lower and upper confidence intervals using a normal distribution,

$$LCI = \bar{x} - (z \times SE),$$

$$UCI = \bar{x} + (z \times SE).$$

Recall from [Chapter 19](#Chapter_19) that a t-score is substituted for a z-score to account for a finite population size.

Suppose we want to calculate 95\% confidence intervals for the ovipositor lengths of SO1 (Figure 35.2A).
There are `r length(sp1)` ovipositor lengths for SO1.

`r format(sp1, nsmall = 3)`


To get the 95\% confidence interval using the method from [Chapter 18](#Chapter_18), we would calculate the mean $\bar{x}_{SO1} =$ `r round(mean(sp1), digits = 3)`, standard deviation $s_{\mathrm{SO1}} =$ `r round(sd(sp1), digits = 3)`, and the t-score for $df = N - 1$ (where $N = 17$), which is $t = 2.120$.
Keeping in mind the formula for standard error $s/\sqrt{N}$, we can calculate,

$$LCI = `r round(mean(sp1), digits = 3)` - \left(2.120 \times \frac{`r round(sd(sp1), digits = 3)`}{\sqrt{17}}\right),$$

$$UCI = `r round(mean(sp1), digits = 3)` + \left(2.120 \times \frac{`r round(sd(sp1), digits = 3)`}{\sqrt{17}}\right).$$

Calculating the above gives us values of LCI = `r round(mean(sp1) - (2.120 * sd(sp1)/sqrt(17)), digits = 3)` and UCI = `r round(mean(sp1) + (2.120 * sd(sp1)/sqrt(17)), digits = 3)`.

Again, randomisation uses a different approach to get an estimate of the same confidence intervals.
Instead of calculating the standard error and multiplying it by a z-score or t-score to encompass a particular interval of probability density, we can instead resample the data we have with replacement many times, calculating the mean each time we resample. 
The general idea is that this process of resampling approximates what would happen if we were to go back and resample new data from our original population many times, thereby giving us the distribution of means from all of these hypothetical resampling events [@Manly2007]. 
To calculate our 95\% confidence intervals, we then only need to rank the calculated means and find the mean closest to the lowest 2.5\% and the highest 97.5\%. 

Remember from [Section 15.3](#sampling-with-and-without-replacement) that the phrase 'resampling with replacement' just means that we are going to randomly sample some values, but not remove them from the pool of possible values after they are sampled. 
If we resample the numbers above with replacement, we might therefore sample some values two or more times by chance, and other values might not be sampled at all. 
The numbers below resample from the above with replacement.

```{r, echo = FALSE, comment = NA}
resamp1 <- sample(sp1, size = length(sp1), replace = TRUE);
```

`r format(resamp1, nsmall = 3)`


Notice that some values appear more than once in the data above, while other values that were present in the original dataset are no longer present after resampling with replacement. Consequently, these new resampled data have a different mean than the original. 
The mean of the original 17 SO1 ovipositor length values was `r round(mean(sp1), digits = 3)`, and the mean of the values resampled above is `r round(mean(resamp1), digits = 3)`. 
We can resample another set of numbers to get a new mean.

```{r, echo = FALSE, comment = NA}
resamp2 <- sample(sp1, size = length(sp1), replace = TRUE);
```

`r format(resamp2, nsmall = 3)`


The mean of the above sample is `r round(mean(resamp2), digits = 3)`. 
We can continue doing this until we have a high number of random samples and means. Figure 35.5 shows the distribution of means if we repeat this process of resampling with replacement and calculating the mean 10000 times.

```{r, echo = FALSE, fig.alt = "A histogram is shown with a mostly bell shape but skewing slightly to the left; x-axis values range from about 2.8 to 3.2.", fig.cap = "A distribution of bootstrapped values of ovipositor lengths from 17 measurements of a fig wasp species collected from Baja, Mexico. A total of 10000 bootstrapped means are shown, and arrows indicate the location of the 2.5% (LCI) and 97.5% (UCI) ranked boostrapped mean values."}
simpleboot <- function(freqs,repli=10000,alpha=0.05){                        
	vals  <- NULL;                                                           
	i     <- 0;                                                            
	while(i < repli){                                                          
		boot  <- sample(x=freqs,size=length(freqs),replace=TRUE);               
		strap <- mean(boot);                                                  
		vals  <- c(vals,strap);                                             
		i     <- i + 1;                                                       
	}                                                                      
	vals   <- sort(x=vals,decreasing=FALSE);                                 
	lowCI  <- vals[round((alpha*0.5)*repli)];                              
	highCI <- vals[round((1-(alpha*0.5))*repli)];                          
	CIs    <- c(lowCI,highCI);                                               
	return(list(vals, CIs));                                                             
} 
booteg <- simpleboot(freqs = sp1);
vals   <- booteg[[1]];
hist(x = vals, main = "", xlab = "Bootstrapped mean values", cex.axis = 1.25,
     cex.lab = 1.25, col = "grey", breaks = 20);
egLCI <- booteg[[2]][1];
egUCI <- booteg[[2]][2];
arrows(x0 = egLCI, x1 = egLCI , y0 = 500, y1 = 10, 
               length = 0.15, lwd = 4, col = "black");
arrows(x0 = egUCI, x1 = egUCI , y0 = 500, y1 = 10, 
               length = 0.15, lwd = 4, col = "black");
text(x = egLCI, y = 630, labels = "LCI", cex = 1.4);
text(x = egUCI, y = 630, labels = "UCI", cex = 1.4);
```

The arrows show the locations of the 2.5\% and 97.5\% ranked values of the bootstrapped means. 
Note that it does not matter if the above distribution is not normal (it appears a bit skewed). 
The bootstrap still works. 
The values using the randomisation approach are LCI = `r round(egLCI, digits = 3)` and UCI = `r round(egUCI, digits = 3)`.
These values are quite similar to those calculated with the traditional approach because we are doing the same thing, conceptually.
But instead of finding confidence intervals of the sample means around the true mean using the t-distribution, we are actually simulating the process of resampling from the population and calculating sample means many times.


## Randomisation conclusions

The examples demonstrated in this chapter are just a small set of what is possible using randomisation approaches.
Randomisation and bootstrapping are highly flexible tools that can be used in a variety of ways [@Manly2007].
They could be used as substitutes (or complements) to any of the hypothesis tests that we have used in this book.
For example, to use a randomisation approach for testing whether or not a correlation coefficient is significantly different from 0, we could randomly shuffle the values of one variable 9999 times.
After each shuffle, we would calculate the Pearson product moment correlation coefficient, then use the 9999 randomised correlations as a null distribution to compare against the observed correlation coefficient.
This would give us a plot like the one shown in Figure 35.4, but showing the null distribution of the correlation coefficient instead of the difference between group means.
Similar approaches could be used for ANOVA or linear regression [@Manly2007].






# _Practical_. Using R {#Chapter_36}

Throughout this book, we have used jamovi for statistical analysis.
Jamovi is excellent software for running simple statistical analyses, but it has its limitations.
Jamovi is built upon the programming language R, which is much more powerful and versatile in comparison.
The R programming langauge is now widely used among scientists for data anslysis.
It can be used to analyse and plot data, run computer simulations, or even write slides, papers, or books (this book, including all interactive applications, were written using R). 
The R programming language is completely free and open source, as is the popular [Rstudio](https://posit.co/downloads/) software for using it. 
The R programming language specialises in statistical computing, which is part of the reason for its popularity among scientists.

Another reason for the popularity of R is its versatility, and the ease with which new techniques can be shared. 
Imagine that you develop a new method for analysing data. 
If you want other researchers to be able to use your method in their research, then you could write your own software from scratch for them to install and use. 
But doing this would be very time consuming, and a lot of that time would likely be spent writing the graphical user interface and making sure that your program worked across platforms (e.g., on Windows and Mac). 
Worse, once written, there would be no easy way to make your program work with other statistical software should you need to integrate different analyses or visualisation tools (e.g., plotting data). 
To avoid all of this, you could instead just present your new method for data analysis and let other researchers write their own code for implementing it. 
But not all researchers will have the time or expertise to write their own code.

Instead, R allows researchers to write new tools for data analysis using simple coding scripts. 
These scripts are organised into R packages, which can be uploaded by authors to the  [Comprehensive R Archive Network](https://cran.r-project.org/) (CRAN), then downloaded by users with a single command in R. 
This way, there is no need for completely different software to be used for different analyses; all analyses can be written and run in R.

The downside to all of this is that learning R can be a bit daunting at first. 
Running analyses is not done by pointing and clicking on icons as in jamovi. 
You need to use code. 
This practical will start with the very basics and work up to some simple data analyses.
Rather than questions to answer, this practical has tasks for you to try to complete in R.

## Getting used to the R interface

There are two ways that you can use R. 
The first way is to download R and Rstudio on your own computer. 
Both R and Rstudio are free to download and use, and work on any major operating system (Windows, Mac, and Linux). 
To download R, go to [the Comprehensive R Archive Network](https://cran.rstudio.com/) and follow the instructions for your operating system (OS).
To download Rstudio, go to [this page on the Rstudio website](https://posit.co/download/rstudio-desktop/#download) and choose the appropriate download for your operating system.

If you cannot or do not want to download R and Rstudio on your own computer, then you can still use both by going to the [Rstudio Cloud](https://rstudio.cloud/) and running Rstudio from a browser such as Firefox or Chrome. 
You will need to click the green "Get Started" button and sign up for a free account. 
When you open Rstudio either on your own computer or the cloud, you will see several windows open, including a console. 
The console will look something like the below.

```
R version 4.2.2 Patched (2022-11-10 r83330) -- "Innocent and Trusting"
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
```

If you click to the right of the greater than sign `>`, you can start using R right in the console. 
You can use R as a standard calculator here to get a feel for the console. 
Try typing something like the below (semi-colons are not actually necessary).
For example, you could add 2 + 5.

```{r, echo = TRUE, comment = NA}
2 + 5;  
```

Try this yourself.

> **Task 1: Add 2 numbers together in R**

You can also multiply in R.

```{r, echo = TRUE, comment = NA}
4 * 4;  
```

The caret key `^` is used for exponents

```{r, echo = TRUE, comment = NA}
5^2;  
```

R will use the correct order of operations (see [Chapter 1](#Chapter_1)) when doing mathematical calculations.
For example, it will know to calculate the exponent before multiplying, and to multiply before adding.

```{r, echo = TRUE, comment = NA}
2 + 4 * 5^2;  
```

Following the correct order of operations, $5^{2} = 25$, which is then multiplied by 4 to get $4 \times 25 = 100$, and we add 2 to get $100 + 2 = 102$.
We can, however, use parentheses to specify a different order of operations.

```{r, echo = TRUE, comment = NA}
(2 + 4) * 5^2;  
```

Now R will calculate $2 + 4 = 6$ and multiply the 6 by 25 to get an answer of 150.
The R console functions very well as a calculator. 
Instead of punching buttons into a hand calculator or mobile phone, an entire equation can be written and placed into the console.
This makes mistakes less likely.
For example, in [Chapter 1.3](#order-of-operations), the following equation was presented,

$$x = 3^{2} + 2(1 + 3)^{2} - 6 \times 0.$$

To solve this in R, we just need to type the full equation in the console.

```{r, echo = TRUE, comment = NA}
3^2 + 2*(1 + 3)^2 - 6 * 0;
```

We get the correct answer of `r 3^2 + 2*(1 + 3)^2 - 6 * 0`. 
Note that there needed to be an asterisk (`*`) to indicate multiplication between the 2 and the left parentheses.
This is because parentheses specify specific functions in R.
We will introduce functions next, but first, try the following task.

> **Task 2: Use the console to calculate $x = \frac{2^2 + 1}{3^2 + 2}$ (hint, you need to put the top and bottom of the fraction in parentheses, e.g., `(2^2 + 1)` for the numerator, and use the forward slash `/` for division).**

You should get an answer of `r (2^2 + 1)/(3^2 +2)`.

As previously mentioned, parentheses specify functions in R.
Functions have specific names such as `sqrt`, which calculates the square root of anything within the parentheses `sqrt()`.
If, for example, you wanted to find the square root of some number, you could use the `sqrt` function below.

```{r, echo = TRUE, comment = NA}
sqrt(256);
```

The console returns the correct answer `r sqrt(256)`. 
Similar functions exist for logarithms (`log`) and trigonometric functions (e.g., `sin`, `cos`). 
The parentheses after the word indicate that some function is being called in R. 
Try to use the square root function to solve for $x$ in another equation.

> **Task 3: Use the console to calculate $\sqrt{3 + 4^2}$.**

You should get an answer of `r sqrt(3 + 4^2)`.

If you type something into the console incorrectly, you do not need to retype it entirely.
You can use the up arrow to scroll through the history of console input.
Try doing that now.

> **Task 4: Use the up arrow to find your previous calculation for $\sqrt{3 + 4^2}$, then change this slightly to calculate $\sqrt{3 + 4^3}$.**

You should get an answer of `r sqrt(3 + 4^3)`.

Functions do not need to be mathematical like the `sqrt` function.
For example, the `getwd` function can be used to let you know what directory (i.e., folder) you are working in.

```{r, echo = TRUE, comment = NA}
getwd();
```

If we were to save or load a file within R, this is the location on the computer from which R would try to load. 
We could also use the `setwd` function to set a new working directory (type the working directory in quotes inside the parentheses: `setwd("folder/subfolder/etc")`). 
You can also set the working directory in Rstudio by going to the toolbar and clicking 'Session' and selecting 'Set Working Directory'.

## Assigning variables in the R console

In R, we can also assign values to variables using the characters `<-` to make an arrow. 
For example, we might want to set `var_1` equal 10.

```{r, echo = TRUE, comment = NA}
var_1 <- 10;
```

We can now use `var_1` in the console.
For example, since `var_1` now equals 10, if we multiply `var_1` by 5, R calculates $10 \times 5 = 50$.

```{r, echo = TRUE, comment = NA}
var_1 * 5; # Multiplying the variable by 5
```

The correct value of `r var_1 * 5` is returned because `var_1` equals `r var_1`. Also note the comment left after the `#` key. 
In R, anything that comes after `#` on a line is a comment that R ignores. 
Comments are ways of explaining in plain words what the code is doing, or drawing attention to important notes about the code.

> **Task 5: Assign a new variable to a value of 4 (call it anything you want), then multiply the new variable by itself.**

For Task 5, you should get an output of 16.

When we assign something to a variable like `var_1`, we are not limited to a single number.
Variables in R can be much more complex.
For example, we can assign a new variable `vector_1` to an ordered set of 6 different numbers using the `c` function.
The `c` function combines values together.

```{r, echo = TRUE, comment = NA}
vector_1 <- c(5, 1, 3, 5, 7, 11); # Six numbers
vector_1;      # Prints out the vector
```

Doing this makes it possible to calculate for all of the values in `vector_1` at the same time.
We might, for example, want to multiply all of the numbers by 10.

```{r, echo = TRUE, comment = NA}
vector_1 * 10
```

Variables also do not necessarily need to be numbers.
We might assign a new variable `phrase_1` to a string of letters.
When doing this, the letters need to be enclosed in quotation marks.

```{r, echo = TRUE, comment = NA}
phrase_1 <- "string of words";
phrase_1;
```

We can even combine numbers, vectors, and words into a single object as a list using the `list` function.

```{r, echo = TRUE, comment = NA}
object_1 <- list(var_1, vector_1, phrase_1);
object_1;
```

There are far too many possibilities to introduce everything about how R works.
The best way to get started is to play around and see what works and what does not.
You will not break anything.
Error messages are good because they can help you learn how R works.

> **Task 6: Use the R console until you find a new error message.**

Try something new in the R console.
Keep going until you get an error message that you have not yet seen, then move on to the next exercise.


## Some descriptive statistics

Now we can get started with some statistical analyses. 
In [Chapter 38](#Chapter_38) we will learn how to do some familiar analyses with R scripts and data uploaded from CSV files, but for now, we will just type our data directly into the console. 
We can use the data from [Chapter 35](#Chapter_35) as an example.
The code below reads in 2 different variables, `SO1` and `SO2`.
The numbers are the same ovipositor lengths from the histograms in Figure 35.2 of [Chapter 35](#Chapter_35).

```{r, echo = TRUE, comment = NA}
SO1 <- c(3.256, 3.133, 3.071, 2.299, 2.995, 2.929, 3.291, 2.658, 3.406, 
         2.976, 2.817, 3.133, 3.000, 3.027, 3.178, 3.133, 3.210);
SO2 <- c(3.014, 2.790, 2.985, 2.911, 2.914, 2.724, 2.967, 2.745, 2.973, 
         2.560, 2.837, 2.883, 2.668, 3.063, 2.639);
```

Copy and paste the code above into the R console.
You should now have 2 new variables, `SO1` and `SO2`

> **Task 7: Type `SO1` into the R console to print the ovipositor lengths for species SO1, then do the same for `SO2`**.

We can replicate the histogram from Figure 35.2A in [Chapter 35](#Chapter_35) with the following code.

```{r, echo = TRUE, fig.alt = "A histogram of SO1 ovipositor lengths that is roughly normally distributed", fig.cap = "Ovipositor length distributions for an unnamed species of fig wasps SO1."}
hist(x = SO1, xlab = "SO1 ovipositor length (mm)", main = "");
```

The bin widths in Figure 36.1 are not the same as Figure 35.2A because there are other options within `hist` that have not been specified.
Within `hist` and other functions, these options such as `x`, `xlab`, and `main` are called *arguments*.
The word 'argument' in this case has nothing to do with a disagreement or logical reasoning.
In this case, it just refers to information that is passed in a function.

> **Task 8: Make a histogram of SO2 ovipositor lengths. Try using the argument `col = "blue"` in the `hist` function to make the histogram bars blue.**

Now we can try collecting some summary statistics for SO1 and SO2.
In the lab practical from [Chapter 14](#Chapter_14), we used jamovi to calculate summary statistics.
Table 36.1 below shows how those same summary statistics can be calculated using functions in R, and the output of each function for SO1.

```{r, echo = FALSE}
Statistic <- c("N", "Std. deviation", "Variance", "Minimum", "Maximum", "Range",
               "IQR", "Mean", "Median");
Function  <- c("`length(SO1)`", "`sd(SO1)`", "`var(SO1)`", "`min(SO1)`",
               "`max(SO1)`", "`range(SO1)`", "`IQR(SO1)`", "`mean(SO1)`",
               "`median(SO1)`");
Output    <- c(round(length(SO1), digits = 0), round(sd(SO1), digits = 6), 
               round(var(SO1), digits = 6), min(SO1), max(SO1), 
               paste(as.character(range(SO1)), collapse = ", "),
               IQR(SO1), round(mean(SO1), digits = 6), median(SO1));
R_table   <- data.frame(Statistic, Function, Output);
knitr::kable(R_table, format = "simple", table.envir = "table",
      caption = "R functions and output for summary statistics applied to a variable (SO1) of ovipositor lengths in an unnamed species of nonpollinating fig wasp.");

```

Notice that the `range` function gives the minimum and maximum of SO1, so we need to subtract the latter from the former to get the actual range.

> **Task 9: Calculate the summary statistics in Table 36.1 for SO2.**

There is no function for standard error in R.
We could write a custom function to calculate the standard error, but for now we can just use the formula from [Chapter 12](#Chapter_12) to calculate the standard error of SO1,

$$SE = \frac{s}{\sqrt{N}}.$$

Since we have the standard deviation ($s$) and sample size ($N$) from Table 36.1, we can calculate SE in R,

```{r, comment = NA}
sd(SO1) / sqrt( length(SO1) );
```


The code above calculates the standard deviation of SO1 (`sd(SO1)`), then divides (`/`) by the square root (`sqrt()`) of the length of SO1 (`length(SO1)`). 
This can be difficult to read because `length(SO1)` is enclosed in `sqrt()`.
But we can break this down step by step to make it easier to read by assigning each component to a new variable.

```{r, comment = NA}
SO1_SD <- sd(SO1);     # Assign the standard deviation
SO1_N  <- length(SO1); # Assign the N
SO1_SD / sqrt(SO1_N);  # Do the calculation for SE
```

We can do the same for SO2.

> **Task 10: Calculate the standard error of SO2 ovipositor lengths.**

With the standard error now calculated, we can also calculate 95 per cent confidence intervals using the formula from [Chapter 18](#Chapter_18), just as we did for SO1 in [Chapter 35](#Chapter_35),

$$LCI = `r round(mean(sp1), digits = 3)` - \left(2.120 \times \frac{`r round(sd(sp1), digits = 3)`}{\sqrt{17}}\right),$$

$$UCI = `r round(mean(sp1), digits = 3)` + \left(2.120 \times \frac{`r round(sd(sp1), digits = 3)`}{\sqrt{17}}\right).$$

Note that `r round(mean(sp1), digits = 3)` is the mean from Table 36.1, and $`r round(sd(sp1), digits = 3)`/\sqrt{17}$ is the standard error that we just calculated for SO1.
The value 2.120 is the t-score associated with df = 17 - 1, i.e., 16 degrees of freedom (see  [Chapter 35](#Chapter_35)).
As in [Chapter 35](#Chapter_35), we get values of LCI = `r round(mean(sp1) - (2.120 * sd(sp1)/sqrt(17)), digits = 3)` and UCI = `r round(mean(sp1) + (2.120 * sd(sp1)/sqrt(17)), digits = 3)`.

> **Task 10: Calculate 95 per cent confidence intervals for SO2 ovipositor lengths (hint: the appropriate t value for df = 14 is 2.145 instead of 2.120).**

In the last exercise, we will attempt to bootstrap these confidence intervals using the method introduced in [Chapter 35](#bootstrapping).


## Bootstrapping confidence intervals

Writing resampling and randomisation procedures in R requires some knowledge of coding.
For now, all that you need to do is copy a pre-defined function into R and use it like any other function.
The function is called `simpleboot`.

```{r}
simpleboot <- function(x, replicates = 1000, CIs = 0.95){
  alpha <- 1 - CIs;
	vals  <- NULL;                                                           
	i     <- 0;                                                            
	while(i < replicates){                                                          
		boot  <- sample(x = x, size = length(x), replace = TRUE);               
		strap <- mean(boot);                                                  
		vals  <- c(vals, strap);                                             
		i     <- i + 1;                                                       
	}                                                                      
	vals   <- sort(x = vals, decreasing = FALSE);                                 
	lowCI  <- vals[round((alpha*0.5)*replicates)];                              
	highCI <- vals[round((1-(alpha*0.5))*replicates)];                          
	confid <- c(lowCI, highCI);                                               
	return(list(vals, confid));                                                             
} 
```


The `simpleboot` function takes a variable (`x`) as an argument, bootstraps mean values `replicates` times, and produces confidence intervals at a level `CIs`.
To use it, highlight and copy the entire function, then put it into the R console.
Make sure that every line from `simpleboot` to the last `}` is copied.
Once it has been placed into the R console, it can be used just like any other function.
For example, we can run `simpleboot` and store the output for SO1 values.

```{r, comment = NA}
SO1_95s <- simpleboot(x = SO1, replicates = 1000, CIs = 0.95);
```

The output will include a list of 2 different objects.
The first object  `SO1_95s[[1]]` will be a list of 1000 bootstrapped mean values, sorted from lowest to highest.
The second object `SO1_95s[[2]]` will be the bootstrapped upper and lower confidence intervals.
First, we can take a look at a histogram of the bootstrapped mean values.

```{r, echo = TRUE, fig.alt = "A histogram bootstrapped mean SO1 values that appears to be roughly normally distributed", fig.cap = "Bootstrapped mean values for 17 ovipositor lengths in an unnamed species of fig wasp collected in Baja, Mexico."}
hist(x = SO1_95s[[1]], xlab = "Bootstrapped SO1 means", main = "");
```

We can then print out the lower and upper confidence intervals, which report the value of the 2.5 per cent rank and 97.5 rank bootstrapped means, respectively.

```{r, comment = NA}
print(SO1_95s[[2]]);
```

Note that these values are not too far off the values of LCI = `r round(mean(sp1) - (2.120 * sd(sp1)/sqrt(17)), digits = 3)` and UCI = `r round(mean(sp1) + (2.120 * sd(sp1)/sqrt(17)), digits = 3)` calculated in the usual way in the previous exercise.

> **Task 11: Use the `simpleboot` function to calculate 95 per cent confidence intervals for SO2 mean ovipositor lengths.**

The practical in [Chapter 38](#Chapter_38) will demonstrate how to read CSV files into R and run hypothesis tests, including t-tests, ANOVAs, chi-square tests, correlation tests, and linear regression.








